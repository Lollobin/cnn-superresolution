{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IABybSFt_Abf"
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111703,
     "status": "ok",
     "timestamp": 1728397170072,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "OTR8hWUC-SX0",
    "outputId": "34068e2d-6d7f-4dc9-da87-cb5cc6c1f886"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ux8qA-hA-Ud8",
    "outputId": "c5761a5d-05e9-4872-a014-335d9ed734ae"
   },
   "outputs": [],
   "source": [
    "working_directory = 'GitHub/dl-superresolution-ipynb'\n",
    "%cd /content/drive/MyDrive/$working_directory\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18342,
     "status": "ok",
     "timestamp": 1728388695293,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "dObxHyJH_f-Q",
    "outputId": "0e6f4310-dcfe-4c81-fd63-e56b9cc943b0"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_sC6ggR_suO"
   },
   "source": [
    "## Git Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EldXDCAATXb"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"e11909390@student.tuwien.ac.at\"\n",
    "!git config --global user.name \"Lollobin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11703,
     "status": "ok",
     "timestamp": 1728388290213,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "Oh3n9PMjBby0",
    "outputId": "3a916314-3a9c-4e19-f7d9-3d16f79a85b0"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1728388879926,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "kgX9MTnv__xc",
    "outputId": "24529da2-c3d3-4123-8022-ecabdfb1999f"
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4084,
     "status": "ok",
     "timestamp": 1728321821916,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "EXqwe0l9ABWA",
    "outputId": "7f01249f-f40f-4a65-a21e-f30653f9cc89"
   },
   "outputs": [],
   "source": [
    "!git commit -a -m \"added connection for google colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3046,
     "status": "ok",
     "timestamp": 1728321831755,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "6j8uohs5AMSU",
    "outputId": "2f87b872-9427-43b6-b139-98941dbeec53"
   },
   "outputs": [],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjVWlSQm8TPi"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Generate patches for training and low res images for validation and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubzGejQl8rsD"
   },
   "source": [
    "### Patch Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "error",
     "timestamp": 1728389383714,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "us157aRN8uAI",
    "outputId": "5b9d886a-38f8-41ea-ee6d-abfca3ea538f"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import patchify\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob as glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "SHOW_PATCHES = False\n",
    "STRIDE = 14\n",
    "SIZE = 32\n",
    "\n",
    "def show_patches(patches):\n",
    "    plt.figure(figsize=(patches.shape[0], patches.shape[1]))\n",
    "    gs = gridspec.GridSpec(patches.shape[0], patches.shape[1])\n",
    "    gs.update(wspace=0.01, hspace=0.02)\n",
    "    counter = 0\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            ax = plt.subplot(gs[counter])\n",
    "            plt.imshow(patches[i, j, 0, :, :, :])\n",
    "            plt.axis('off')\n",
    "            counter += 1\n",
    "    plt.show()\n",
    "\n",
    "def create_patches(\n",
    "    input_paths, out_hr_path, out_lr_path,\n",
    "):\n",
    "    os.makedirs(out_hr_path, exist_ok=True)\n",
    "    os.makedirs(out_lr_path, exist_ok=True)\n",
    "    all_paths = []\n",
    "    for input_path in input_paths:\n",
    "        all_paths.extend(glob.glob(f\"{input_path}/*\"))\n",
    "    print(f\"Creating patches for {len(all_paths)} images\")\n",
    "    for image_path in tqdm(all_paths, total=len(all_paths)):\n",
    "        image = Image.open(image_path)\n",
    "        image_name = image_path.split(os.path.sep)[-1].split('.')[0]\n",
    "        w, h = image.size\n",
    "        # Create patches of size (32, 32, 3)\n",
    "        patches = patchify.patchify(np.array(image), (32, 32, 3), STRIDE)\n",
    "        if SHOW_PATCHES:\n",
    "            show_patches(patches)\n",
    "        counter = 0\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                counter += 1\n",
    "                patch = patches[i, j, 0, :, :, :]\n",
    "                patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_hr_path}/{image_name}_{counter}.png\",\n",
    "                    patch\n",
    "                )\n",
    "                # Convert to bicubic and save.\n",
    "                h, w, _ = patch.shape\n",
    "                low_res_img = cv2.resize(patch, (int(w*0.5), int(h*0.5)),\n",
    "                                        interpolation=cv2.INTER_CUBIC)\n",
    "                # Now upscale using BICUBIC.\n",
    "                high_res_upscale = cv2.resize(low_res_img, (w, h),\n",
    "                                            interpolation=cv2.INTER_CUBIC)\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_lr_path}/{image_name}_{counter}.png\",\n",
    "                    high_res_upscale\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_patches(['input/T91'], 'input/t91_hr_patches', 'input/t91_lr_patches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bicubic Scaling for Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfGA9wyqAHH6"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob as glob\n",
    "import os\n",
    "\n",
    "paths = ['input/Set14/original', 'input/Set5/original']\n",
    "scale_factor = '2x' # options 2x, 3x, 4x\n",
    "images = []\n",
    "\n",
    "for path in paths:\n",
    "    images.extend(glob.glob(f\"{path}/*.png\"))\n",
    "print(len(images))\n",
    "# Select scaling-factor and set up directories according to that.\n",
    "if scale_factor == '2x':\n",
    "    scale_factor = 0.5\n",
    "    os.makedirs('input/test_bicubic_rgb_2x', exist_ok=True)\n",
    "    save_path_lr = 'input/test_bicubic_rgb_2x'\n",
    "    os.makedirs('input/test_hr', exist_ok=True)\n",
    "    save_path_hr = 'input/test_hr'\n",
    "if scale_factor == '3x':\n",
    "    scale_factor = 0.333\n",
    "    os.makedirs('input/test_bicubic_rgb_3x', exist_ok=True)\n",
    "    os.makedirs('input/test_hr', exist_ok=True)\n",
    "    save_path_lr = 'input/test_bicubic_rgb_3x'\n",
    "    save_path_hr = 'input/test_hr'\n",
    "if scale_factor == '4x':\n",
    "    scale_factor = 0.25\n",
    "    os.makedirs('input/test_bicubic_rgb_4x', exist_ok=True)\n",
    "    os.makedirs('input/test_hr', exist_ok=True)\n",
    "    save_path_lr = 'input/test_bicubic_rgb_4x'\n",
    "    save_path_hr = 'input/test_hr'\n",
    "print(f\"Scaling factor: {scale_factor}\")\n",
    "print(f\"Low resolution images save path: {save_path_lr}\")\n",
    "for image in images:\n",
    "    orig_img = Image.open(image)\n",
    "    image_name = image.split(os.path.sep)[-1]\n",
    "    w, h = orig_img.size[:]\n",
    "    print(f\"Original image dimensions: {w}, {h}\")\n",
    "    orig_img.save(f\"{save_path_hr}/{image_name}\")\n",
    "    low_res_img = orig_img.resize((int(w*scale_factor), int(h*scale_factor)), Image.BICUBIC)\n",
    "    # Upscale using BICUBIC.\n",
    "    high_res_upscale = low_res_img.resize((w, h), Image.BICUBIC)\n",
    "    high_res_upscale.save(f\"{save_path_lr}/{image_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df1rTJmG-dp-"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Define utility functions that are used later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hvJVv0i-o2P"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "plt.style.use('ggplot')\n",
    "def psnr(label, outputs, max_val=1.):\n",
    "    \"\"\"\n",
    "    Compute Peak Signal to Noise Ratio (the higher the better).\n",
    "    PSNR = 20 * log10(MAXp) - 10 * log10(MSE).\n",
    "    https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition\n",
    "    Note that the output and label pixels (when dealing with images) should\n",
    "    be normalized as the `max_val` here is 1 and not 255.\n",
    "    \"\"\"\n",
    "    label = label.cpu().detach().numpy()\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    diff = outputs - label\n",
    "    rmse = math.sqrt(np.mean((diff) ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    else:\n",
    "        PSNR = 20 * math.log10(max_val / rmse)\n",
    "        return PSNR\n",
    "\n",
    "def save_plot(train_loss, val_loss, train_psnr, val_psnr):\n",
    "    \n",
    "    # Loss plots.\n",
    "    print('Saving loss plots...')\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(val_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # PSNR plots.\n",
    "    print('Saving PSNR plots...')\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_psnr, color='green', label='train PSNR dB')\n",
    "    plt.plot(val_psnr, color='blue', label='validataion PSNR dB')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.legend()\n",
    "    plt.savefig('outputs/psnr.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_model_state(model):\n",
    "    # save the model to disk\n",
    "    print('Saving model state...')\n",
    "    torch.save(model.state_dict(), 'outputs/model.pth')\n",
    "\n",
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    # Remove the last model checkpoint if present.\n",
    "    torch.save({\n",
    "                'epoch': epochs+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, f\"outputs/model_ckpt.pth\")\n",
    "\n",
    "def save_validation_results(outputs, epoch, batch_iter):\n",
    "    \"\"\"\n",
    "    Function to save the validation reconstructed images.\n",
    "    \"\"\"\n",
    "    save_image(\n",
    "        outputs,\n",
    "        f\"outputs/valid_results/val_sr_{epoch}_{batch_iter}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob as glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "transform_image = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transform_label = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "class ResNetSRCNNDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "        self.all_label_paths = glob.glob(f\"{label_paths}/*\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.all_image_paths))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.all_image_paths[index]).convert('RGB')\n",
    "        label = Image.open(self.all_label_paths[index]).convert('RGB')\n",
    "\n",
    "        image = transform_image(image)\n",
    "        label = transform_label(label)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "# Prepare the datasets.\n",
    "def get_datasets(\n",
    "    train_image_paths, train_label_paths,\n",
    "    valid_image_path, valid_label_paths\n",
    "):\n",
    "    dataset_train = ResNetSRCNNDataset(\n",
    "        train_image_paths, train_label_paths\n",
    "    )\n",
    "    dataset_valid = ResNetSRCNNDataset(\n",
    "        valid_image_path, valid_label_paths\n",
    "    )\n",
    "    return dataset_train, dataset_valid\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_dataloaders(dataset_train, dataset_valid):\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, \n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, \n",
    "        batch_size=TEST_BATCH_SIZE,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_loader, valid_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1,2,0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "TRAIN_LABEL_PATHS = 'input/t91_hr_patches'\n",
    "TRAN_IMAGE_PATHS = 'input/t91_lr_patches'\n",
    "VALID_LABEL_PATHS = 'input/test_hr'\n",
    "VALID_IMAGE_PATHS = 'input/test_bicubic_rgb_2x'\n",
    "\n",
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS,\n",
    "    VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "\n",
    "#images = next(iter(train_loader))\n",
    "images = next(iter(valid_loader))\n",
    "\n",
    "imshow(images[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "class ResNetSRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetSRCNN, self).__init__()\n",
    "        \n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.resnet_layers = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,    # Batch normalization\n",
    "            resnet.relu,   # Activation\n",
    "            resnet.layer1  # First residual block (without downsampling)\n",
    "        )\n",
    "        \n",
    "        # Freeze the ResNet layers\n",
    "        for param in self.resnet_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,64,kernel_size=4,stride=2,padding=1), # Upscaling to keep input and output size the same\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Keep the spatial size same\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, padding=1),   # Output RGB channels\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor=2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode='reflect')\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.resnet_layers(x)\n",
    "        \n",
    "        # Reconstruction\n",
    "        x = self.reconstruction(features)\n",
    "        \n",
    "        # Remove padding\n",
    "        x = x[:, :, :original_size[0], :original_size[1]]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "class model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model1, self).__init__()\n",
    "        \n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.resnet_layers = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,    # Batch normalization\n",
    "            resnet.relu,   # Activation\n",
    "            #resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "        )\n",
    "        \n",
    "        # Freeze the ResNet layers\n",
    "        for param in self.resnet_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,32,kernel_size=3,stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor=2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode='reflect')\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.resnet_layers(x)\n",
    "        \n",
    "        # Reconstruction\n",
    "        x = self.reconstruction(features)\n",
    "        \n",
    "        # Remove padding\n",
    "        x = x[:, :, :original_size[0], :original_size[1]]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "class model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model2, self).__init__()\n",
    "        \n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.resnet_layers = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,    # Batch normalization\n",
    "            resnet.relu,   # Activation\n",
    "            #resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "            resnet.layer2  # Second residual block (with downsampling)\n",
    "        )\n",
    "        \n",
    "        # Freeze the ResNet layers\n",
    "        for param in self.resnet_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128,64,kernel_size=3,stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64,32,kernel_size=3,stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor=4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode='reflect')\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.resnet_layers(x)\n",
    "        \n",
    "        # Reconstruction\n",
    "        x = self.reconstruction(features)\n",
    "        \n",
    "        # Remove padding\n",
    "        x = x[:, :, :original_size[0], :original_size[1]]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "class model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model3, self).__init__()\n",
    "        \n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.resnet_layers = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,    # Batch normalization\n",
    "            resnet.relu,   # Activation\n",
    "            #resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual bloc\n",
    "            resnet.layer2,  # Second residual bloc\n",
    "            resnet.layer3  # Third residual block\n",
    "        )\n",
    "        \n",
    "        # Freeze the ResNet layers\n",
    "        for param in self.resnet_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.reconstruction = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256,128,kernel_size=3,stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128,64,kernel_size=3,stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64,32,kernel_size=3,stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor=8\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode='reflect')\n",
    "        \n",
    "        # Feature extraction\n",
    "        features = self.resnet_layers(x)\n",
    "        \n",
    "        # Reconstruction\n",
    "        x = self.reconstruction(features)\n",
    "        \n",
    "        # Remove padding\n",
    "        x = x[:, :, :original_size[0], :original_size[1]]\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "# Create the model\n",
    "model_dummy = model1()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_dummy = model_dummy.to(device)\n",
    "\n",
    "# Example forward pass with a dummy input\n",
    "dummy_input = torch.randn(1, 3, 32,32).to(device)\n",
    "\n",
    "resnet_output = model_dummy.resnet_layers(dummy_input)\n",
    "reconstruction_output = model_dummy.reconstruction(resnet_output)\n",
    "output = model_dummy(dummy_input)\n",
    "\n",
    "# Output shape should match the input shape\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"ResNet output shape: {resnet_output.shape}\")\n",
    "print(f\"Reconstruction output shape: {reconstruction_output.shape}\")\n",
    "print(f\"Final output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {count_parameters(model_dummy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Constants\n",
    "TRAIN_LABEL_PATHS = 'input/t91_hr_patches'\n",
    "TRAN_IMAGE_PATHS = 'input/t91_lr_patches'\n",
    "VALID_LABEL_PATHS = 'input/test_hr'\n",
    "VALID_IMAGE_PATHS = 'input/test_bicubic_rgb_2x'\n",
    "SAVE_VALIDATION_RESULTS = True\n",
    "\n",
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS,\n",
    "    VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "\n",
    "print(f\"Training samples: {len(dataset_train)}\")\n",
    "print(f\"Validation samples: {len(dataset_valid)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(1, 3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor(std).view(1, 3, 1, 1).to(tensor.device)\n",
    "    tensor = tensor * std + mean\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "\n",
    "# Generic function to train a model\n",
    "def train_model(model, criterion, optimizer=None, scheduler=None, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    # Copy weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    train_psnr, val_psnr = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "               # if scheduler is not None:\n",
    "                    #scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = valid_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_psnr = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                inputs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                if(optimizer is not None):\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs_denorm = denormalize(outputs)\n",
    "                    loss = criterion(outputs_denorm, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_psnr += psnr(labels, outputs_denorm)\n",
    "\n",
    "            epoch_loss = running_loss /len(dataloader)\n",
    "            epoch_psnr = running_psnr /len(dataloader)\n",
    "\n",
    "            print('{} Loss: {:.4f} PSNR: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_psnr))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_psnr > best_psnr:\n",
    "                best_psnr = epoch_psnr\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_psnr.append(epoch_psnr)\n",
    "            \n",
    "            if phase == \"val\":\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_psnr.append(epoch_psnr)\n",
    "\n",
    "        if(scheduler is not None):\n",
    "            scheduler.step(epoch_loss)\n",
    "            \n",
    "        # save state and plots every 10 epochs\n",
    "        if(epoch+1)%10==0 or epoch == num_epochs-1:\n",
    "            save_model_state(model)\n",
    "            save_plot(train_loss, val_loss, train_psnr, val_psnr)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val PSNR: {:4f}'.format(best_psnr))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model1().to(device)\n",
    "epochs = 200\n",
    "lr = 0.001\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "#print(torch.cuda.is_available())\n",
    "#print(next(model.parameters()).device)\n",
    "\n",
    "model = train_model(model, criterion, optimizer,scheduler,num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading and upscaling a single image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = 11\n",
    "valid_images = valid_loader.dataset[index]\n",
    "\n",
    "lr_image = denormalize(valid_images[0].unsqueeze(0)).to(device)\n",
    "hr_image = valid_images[1].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr_image = denormalize(model(valid_images[0].unsqueeze(0).to(device)))\n",
    "\n",
    "lr_psnr = psnr(hr_image,lr_image)\n",
    "sr_psnr = psnr(sr_image,hr_image)\n",
    "\n",
    "lr_image = lr_image.cpu()[0].numpy().transpose((1,2,0))\n",
    "hr_image = hr_image.cpu()[0].numpy().transpose((1,2,0))\n",
    "sr_image = sr_image.cpu()[0].numpy().transpose((1,2,0))\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title(f\"Low Resolution (Bicubic Scaling), PSNR {lr_psnr:.2f}\")\n",
    "plt.imshow(lr_image)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f\"Super Resolution (CNN Scaling), PSNR {sr_psnr:.2f}\")\n",
    "plt.imshow(sr_image)\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(\"High Resolution (Label)\")\n",
    "plt.imshow(hr_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sets of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            image_data = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "            outputs = model(image_data)\n",
    "            \n",
    "            outputs = denormalize(outputs)\n",
    "            \n",
    "            # Calculate batch psnr (once every `batch_size` iterations).\n",
    "            batch_psnr = psnr(label, outputs)\n",
    "            running_psnr += batch_psnr\n",
    "\n",
    "    final_loss = running_loss/len(dataloader.dataset)\n",
    "    final_psnr = running_psnr/len(dataloader)\n",
    "    return final_loss, final_psnr\n",
    "\n",
    "\n",
    "# The SRCNN dataset module.\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.all_image_paths))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The high resolution ground truth label.\n",
    "        label = Image.open(self.all_image_paths[index]).convert('RGB')\n",
    "        w, h = label.size[:]\n",
    "\n",
    "        # Convert to 2x bicubic.\n",
    "        low_res_img = label.resize((int(w*0.5), int(h*0.5)), Image.BICUBIC)\n",
    "        # The low resolution input image.\n",
    "        image = low_res_img.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "\n",
    "\n",
    "        image /= 255.\n",
    "        label /= 255.\n",
    "\n",
    "        # normalize lr image\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225]) \n",
    "        image = (image - mean)/std\n",
    "\n",
    "        image = image.transpose([2, 0, 1])\n",
    "        label = label.transpose([2, 0, 1])\n",
    "\n",
    "        return (\n",
    "            torch.tensor(image, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "# Prepare the datasets.\n",
    "def get_test_datasets(\n",
    "    image_paths\n",
    "):\n",
    "    dataset_test = TestDataset(image_paths)\n",
    "    return dataset_test\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_test_dataloaders(dataset_test):\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test, \n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = model1().to(device)\n",
    "model.load_state_dict(torch.load('outputs/model.pth'))\n",
    "data_paths = [\n",
    "    ['input/Set5/original', 'Set5'],\n",
    "    ['input/Set14/original', 'Set14']\n",
    "]\n",
    "for data_path in data_paths:\n",
    "    dataset_test = get_test_datasets(data_path[0])\n",
    "    test_loader = get_test_dataloaders(dataset_test)\n",
    "    _, test_psnr = validate(model, test_loader, device)\n",
    "    print(f\"Test PSNR on {data_path[1]}: {test_psnr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
