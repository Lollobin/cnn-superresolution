{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IABybSFt_Abf"
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111703,
     "status": "ok",
     "timestamp": 1728397170072,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "OTR8hWUC-SX0",
    "outputId": "34068e2d-6d7f-4dc9-da87-cb5cc6c1f886"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ux8qA-hA-Ud8",
    "outputId": "c5761a5d-05e9-4872-a014-335d9ed734ae"
   },
   "outputs": [],
   "source": [
    "working_directory = 'GitHub/dl-superresolution-ipynb'\n",
    "%cd /content/drive/MyDrive/$working_directory\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18342,
     "status": "ok",
     "timestamp": 1728388695293,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "dObxHyJH_f-Q",
    "outputId": "0e6f4310-dcfe-4c81-fd63-e56b9cc943b0"
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_sC6ggR_suO"
   },
   "source": [
    "## Git Management\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EldXDCAATXb"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"e11909390@student.tuwien.ac.at\"\n",
    "!git config --global user.name \"Lollobin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11703,
     "status": "ok",
     "timestamp": 1728388290213,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "Oh3n9PMjBby0",
    "outputId": "3a916314-3a9c-4e19-f7d9-3d16f79a85b0"
   },
   "outputs": [],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1728388879926,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "kgX9MTnv__xc",
    "outputId": "24529da2-c3d3-4123-8022-ecabdfb1999f"
   },
   "outputs": [],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4084,
     "status": "ok",
     "timestamp": 1728321821916,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "EXqwe0l9ABWA",
    "outputId": "7f01249f-f40f-4a65-a21e-f30653f9cc89"
   },
   "outputs": [],
   "source": [
    "!git commit -a -m \"added connection for google colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3046,
     "status": "ok",
     "timestamp": 1728321831755,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "6j8uohs5AMSU",
    "outputId": "2f87b872-9427-43b6-b139-98941dbeec53"
   },
   "outputs": [],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjVWlSQm8TPi"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Generate patches for training and low res images for validation and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubzGejQl8rsD"
   },
   "source": [
    "### Patch Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "error",
     "timestamp": 1728389383714,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "us157aRN8uAI",
    "outputId": "5b9d886a-38f8-41ea-ee6d-abfca3ea538f"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import patchify\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob as glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "SHOW_PATCHES = False\n",
    "STRIDE = 114\n",
    "SIZE = 224\n",
    "SCALE = 4.0  # Upscale factor 2, 3 or 4\n",
    "\n",
    "\n",
    "def show_patches(patches):\n",
    "    plt.figure(figsize=(patches.shape[0], patches.shape[1]))\n",
    "    gs = gridspec.GridSpec(patches.shape[0], patches.shape[1])\n",
    "    gs.update(wspace=0.01, hspace=0.02)\n",
    "    counter = 0\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            ax = plt.subplot(gs[counter])\n",
    "            plt.imshow(patches[i, j, 0, :, :, :])\n",
    "            plt.axis(\"off\")\n",
    "            counter += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_patches(\n",
    "    input_paths,\n",
    "    out_hr_path,\n",
    "    out_lr_path,\n",
    "):\n",
    "    os.makedirs(out_hr_path, exist_ok=True)\n",
    "    os.makedirs(out_lr_path, exist_ok=True)\n",
    "    all_paths = []\n",
    "    for input_path in input_paths:\n",
    "        all_paths.extend(glob.glob(f\"{input_path}/*\"))\n",
    "    print(f\"Creating patches for {len(all_paths)} images\")\n",
    "    for image_path in tqdm(all_paths, total=len(all_paths)):\n",
    "        image = Image.open(image_path)\n",
    "        image_name = image_path.split(os.path.sep)[-1].split(\".\")[0]\n",
    "        w, h = image.size\n",
    "        # Create patches of width and height SIZE.\n",
    "        patches = patchify.patchify(np.array(image), (SIZE, SIZE, 3), STRIDE)\n",
    "        if SHOW_PATCHES:\n",
    "            show_patches(patches)\n",
    "        counter = 0\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                counter += 1\n",
    "                patch = patches[i, j, 0, :, :, :]\n",
    "                patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(f\"{out_hr_path}/{image_name}_{counter}.png\", patch)\n",
    "                # Convert to bicubic and save.\n",
    "                h, w, _ = patch.shape\n",
    "                low_res_img = cv2.resize(\n",
    "                    patch,\n",
    "                    (int(w * (1.0 / SCALE)), int(h * 1.0 / SCALE)),\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                )\n",
    "                # Now upscale using BICUBIC.\n",
    "                high_res_upscale = cv2.resize(\n",
    "                    low_res_img, (w, h), interpolation=cv2.INTER_CUBIC\n",
    "                )\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_lr_path}/{image_name}_{counter}.png\", high_res_upscale\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_patches([\"input/PIRM\"], \"input/PIRM_hr_patches_4x\", \"input/PIRM_lr_patches_4x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bicubic Scaling for Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfGA9wyqAHH6"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob as glob\n",
    "import os\n",
    "\n",
    "paths = [\"input/Set14/original\", \"input/Set5/original\"]\n",
    "scale_factor = \"4x\"  # options 2x, 3x, 4x\n",
    "images = []\n",
    "\n",
    "for path in paths:\n",
    "    images.extend(glob.glob(f\"{path}/*.png\"))\n",
    "print(len(images))\n",
    "# Select scaling-factor and set up directories according to that.\n",
    "if scale_factor == \"2x\":\n",
    "    scale_factor = 0.5\n",
    "    os.makedirs(\"input/test_bicubic_rgb_2x\", exist_ok=True)\n",
    "    save_path_lr = \"input/test_bicubic_rgb_2x\"\n",
    "    os.makedirs(\"input/test_hr\", exist_ok=True)\n",
    "    save_path_hr = \"input/test_hr\"\n",
    "if scale_factor == \"3x\":\n",
    "    scale_factor = 0.333\n",
    "    os.makedirs(\"input/test_bicubic_rgb_3x\", exist_ok=True)\n",
    "    os.makedirs(\"input/test_hr\", exist_ok=True)\n",
    "    save_path_lr = \"input/test_bicubic_rgb_3x\"\n",
    "    save_path_hr = \"input/test_hr\"\n",
    "if scale_factor == \"4x\":\n",
    "    scale_factor = 0.25\n",
    "    os.makedirs(\"input/test_bicubic_rgb_4x\", exist_ok=True)\n",
    "    os.makedirs(\"input/test_hr\", exist_ok=True)\n",
    "    save_path_lr = \"input/test_bicubic_rgb_4x\"\n",
    "    save_path_hr = \"input/test_hr\"\n",
    "print(f\"Scaling factor: {scale_factor}\")\n",
    "print(f\"Low resolution images save path: {save_path_lr}\")\n",
    "for image in images:\n",
    "    orig_img = Image.open(image)\n",
    "    image_name = image.split(os.path.sep)[-1]\n",
    "    w, h = orig_img.size[:]\n",
    "    print(f\"Original image dimensions: {w}, {h}\")\n",
    "    orig_img.save(f\"{save_path_hr}/{image_name}\")\n",
    "    low_res_img = orig_img.resize(\n",
    "        (int(w * scale_factor), int(h * scale_factor)), Image.BICUBIC\n",
    "    )\n",
    "    # Upscale using BICUBIC.\n",
    "    high_res_upscale = low_res_img.resize((w, h), Image.BICUBIC)\n",
    "    high_res_upscale.save(f\"{save_path_lr}/{image_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df1rTJmG-dp-"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Define utility functions that are used later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hvJVv0i-o2P"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "def psnr(label, outputs, max_val=1.0):\n",
    "    \"\"\"\n",
    "    Compute Peak Signal to Noise Ratio (the higher the better).\n",
    "    PSNR = 20 * log10(MAXp) - 10 * log10(MSE).\n",
    "    https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition\n",
    "    Note that the output and label pixels (when dealing with images) should\n",
    "    be normalized as the `max_val` here is 1 and not 255.\n",
    "    \"\"\"\n",
    "    label = label.cpu().detach().numpy()\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    diff = outputs - label\n",
    "    rmse = math.sqrt(np.mean((diff) ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    else:\n",
    "        PSNR = 20 * math.log10(max_val / rmse)\n",
    "        return PSNR\n",
    "\n",
    "\n",
    "def save_plot(train_loss, val_loss, train_psnr, val_psnr):\n",
    "\n",
    "    # Loss plots.\n",
    "    print(\"Saving loss plots...\")\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color=\"orange\", label=\"train loss\")\n",
    "    plt.plot(val_loss, color=\"red\", label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"outputs/loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # PSNR plots.\n",
    "    print(\"Saving PSNR plots...\")\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_psnr, color=\"green\", label=\"train PSNR dB\")\n",
    "    plt.plot(val_psnr, color=\"blue\", label=\"validation PSNR dB\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"PSNR (dB)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"outputs/psnr.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model_state(model):\n",
    "    # save the model to disk\n",
    "    print(\"Saving model state...\")\n",
    "    torch.save(model.state_dict(), \"outputs/model.pth\")\n",
    "\n",
    "\n",
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    # Remove the last model checkpoint if present.\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epochs + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": criterion,\n",
    "        },\n",
    "        f\"outputs/model_ckpt.pth\",\n",
    "    )\n",
    "\n",
    "\n",
    "def save_validation_results(outputs, epoch, batch_iter):\n",
    "    \"\"\"\n",
    "    Function to save the validation reconstructed images.\n",
    "    \"\"\"\n",
    "    save_image(outputs, f\"outputs/valid_results/val_sr_{epoch}_{batch_iter}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "transform_image = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_label = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ResNetSRCNNDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "        self.all_label_paths = glob.glob(f\"{label_paths}/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.all_image_paths[index]).convert(\"RGB\")\n",
    "        label = Image.open(self.all_label_paths[index]).convert(\"RGB\")\n",
    "\n",
    "        image = transform_image(image)\n",
    "        label = transform_label(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Prepare the datasets.\n",
    "def get_datasets(\n",
    "    train_image_paths, train_label_paths, valid_image_path, valid_label_paths\n",
    "):\n",
    "    dataset_train = ResNetSRCNNDataset(train_image_paths, train_label_paths)\n",
    "    dataset_valid = ResNetSRCNNDataset(valid_image_path, valid_label_paths)\n",
    "    return dataset_train, dataset_valid\n",
    "\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_dataloaders(dataset_train, dataset_valid):\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, batch_size=TEST_BATCH_SIZE, shuffle=False, pin_memory=True\n",
    "    )\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "\n",
    "TRAIN_LABEL_PATHS = \"input/PIRM_hr_patches_224\"\n",
    "TRAN_IMAGE_PATHS = \"input/PIRM_lr_patches_224\"\n",
    "VALID_LABEL_PATHS = \"input/test_hr\"\n",
    "VALID_IMAGE_PATHS = \"input/test_bicubic_rgb_2x\"\n",
    "\n",
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS, VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "\n",
    "images = next(iter(train_loader))\n",
    "# images = next(iter(valid_loader))\n",
    "\n",
    "imshow(images[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSR1\n",
    "\n",
    "Contains 1 ResNet block in the encoder.\n",
    "\n",
    "Trainable parameters (decoder): 23.523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR1, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            # resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR1_m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR1_m, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=0, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                32, 16, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSR2\n",
    "\n",
    "Contains 2 ResNet blocks in the encoder.\n",
    "\n",
    "Trainable parameters (decoder): 97.315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR2, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "            resnet.layer2,  # Second residual block (with downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR2_m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR2_m, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "            resnet.layer2,  # Second residual block (with downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSR3\n",
    "\n",
    "Contains 3 ResNet blocks in the encoder.\n",
    "\n",
    "Trainable parameters (decoder): 392.355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR3, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            # resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual bloc\n",
    "            resnet.layer2,  # Second residual bloc\n",
    "            resnet.layer3,  # Third residual block\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 8\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=(1, 1), padding=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=(1, 1), padding=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=(1, 1), padding=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGRS1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR1p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR1p, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR1p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR1p, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "print(vgg)\n",
    "\n",
    "\n",
    "class VGGSR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR2, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "            vgg.features[13],  # Max Pooling\n",
    "            vgg.features[14],  # Convolutional layer\n",
    "            vgg.features[15],\n",
    "            vgg.features[16],\n",
    "            # vgg.features[17],\n",
    "            # vgg.features[18],\n",
    "            # vgg.features[19],\n",
    "            # vgg.features[20],\n",
    "            # vgg.features[21],\n",
    "            # vgg.features[22],\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 3, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "print(vgg)\n",
    "\n",
    "\n",
    "class VGGSR2p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR2p, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "            vgg.features[13],  # Max Pooling\n",
    "            vgg.features[14],  # Convolutional layer\n",
    "            vgg.features[15],\n",
    "            vgg.features[16],\n",
    "            # vgg.features[17],\n",
    "            # vgg.features[18],\n",
    "            # vgg.features[19],\n",
    "            # vgg.features[20],\n",
    "            # vgg.features[21],\n",
    "            # vgg.features[22],\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR0, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchscan import summary\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model_dummy = VGGSR2p()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dummy = model_dummy.to(device)\n",
    "\n",
    "for param in model_dummy.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Example forward pass with a dummy input\n",
    "dummy_input = torch.randn(1, 3, 224, 221).to(device)\n",
    "encoder_output = model_dummy.encoder(dummy_input)\n",
    "decoder_output = model_dummy.decoder(encoder_output)\n",
    "output = model_dummy(dummy_input)\n",
    "\n",
    "# Output shape should match the input shape\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "print(f\"Final output shape: {output.shape}\")\n",
    "print(\n",
    "    f\"Number of parameters (encoder + decoder ): {count_parameters(model_dummy.encoder)}+{count_parameters(model_dummy.decoder)} = {count_parameters(model_dummy)}\"\n",
    ")\n",
    "\n",
    "\n",
    "summary(model_dummy, (3, 224, 221), receptive_field=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Constants\n",
    "# TRAIN_LABEL_PATHS = 'input/PIRM_hr_patches_4x'\n",
    "# TRAN_IMAGE_PATHS = 'input/PIRM_lr_patches_4x'\n",
    "TRAIN_LABEL_PATHS = \"input/T91_hr_patches_32_x4\"\n",
    "TRAN_IMAGE_PATHS = \"input/T91_lr_patches_32_x4\"\n",
    "VALID_LABEL_PATHS = \"input/test_hr\"\n",
    "VALID_IMAGE_PATHS = \"input/test_bicubic_rgb_4x\"\n",
    "SAVE_VALIDATION_RESULTS = True\n",
    "\n",
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS, VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "\n",
    "print(f\"Training samples: {len(dataset_train)}\")\n",
    "print(f\"Validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(1, 3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor(std).view(1, 3, 1, 1).to(tensor.device)\n",
    "    tensor = tensor * std + mean\n",
    "    return torch.clamp(tensor, 0, 1)\n",
    "\n",
    "\n",
    "# Generic function to train a model\n",
    "def train_model(model, criterion, optimizer=None, scheduler=None, num_epochs=10):\n",
    "    since = time.time()\n",
    "\n",
    "    # Copy weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    train_psnr, val_psnr = [], []\n",
    "\n",
    "    lr_list = []\n",
    "    if scheduler is not None:\n",
    "        lr_list.append((1, scheduler.get_last_lr()[0]))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                # if scheduler is not None:\n",
    "                # scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                dataloader = valid_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_psnr = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                inputs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                if optimizer is not None:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = denormalize(outputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_psnr += psnr(labels, outputs)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_psnr = running_psnr / len(dataloader)\n",
    "\n",
    "            print(\"{} Loss: {:.4f} PSNR: {:.4f}\".format(phase, epoch_loss, epoch_psnr))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_psnr > best_psnr:\n",
    "                best_psnr = epoch_psnr\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_psnr.append(epoch_psnr)\n",
    "\n",
    "            if phase == \"val\":\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_psnr.append(epoch_psnr)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            last_lr = scheduler.get_last_lr()[0]\n",
    "            scheduler.step(epoch_loss)\n",
    "            new_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            if last_lr != new_lr:\n",
    "                print(\"LR changed from \", last_lr, \" to \", new_lr)\n",
    "                lr_list.append((epoch + 1, new_lr))\n",
    "\n",
    "        # save state and plots every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "            save_model_state(model)\n",
    "            save_plot(train_loss, val_loss, train_psnr, val_psnr)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    print(\"Best val PSNR: {:4f}\".format(best_psnr))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # write PSNR values to text file for LaTeX\n",
    "    with open(\"outputs/psnr.txt\", \"w\") as f:\n",
    "        f.write(\"Train PSNR values: \\n\")\n",
    "        for epoch, psnr_value in enumerate(train_psnr):\n",
    "            f.write(f\"({epoch+1}, {psnr_value:.4f})\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Validation PSNR values: \\n\")\n",
    "        for epoch, psnr_value in enumerate(val_psnr):\n",
    "            f.write(f\"({epoch+1}, {psnr_value:.4f})\")\n",
    "\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Learning rate values: \\n\")\n",
    "        for epoch, lr_value in lr_list:\n",
    "            f.write(f\"({epoch+1}, {lr_value})\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "model = VGGSR1p().to(device)\n",
    "# model.load_state_dict(torch.load(\"outputs/experiments_final/VGGSR_e150_model.pth\"))\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 500\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = None\n",
    "\n",
    "# final_lr = 0.00001\n",
    "# gamma = (final_lr / lr) ** (1 / epochs)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.1, patience=25\n",
    ")\n",
    "\n",
    "\n",
    "# Freeze the encoder layers\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "model = VGGSR1p().to(device)\n",
    "model.load_state_dict(torch.load(\"outputs/experiments_final/model.pth\"))\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "\n",
    "# Unfreeze the encoder layers\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "model_fine_tuned = train_model(\n",
    "\n",
    "    model, criterion, optimizer, scheduler, num_epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading and upscaling a single image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "valid_images = valid_loader.dataset[index]\n",
    "\n",
    "lr_image = denormalize(valid_images[0].unsqueeze(0)).to(device)\n",
    "hr_image = valid_images[1].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr_image = denormalize(model(valid_images[0].unsqueeze(0).to(device)))\n",
    "\n",
    "lr_psnr = psnr(hr_image, lr_image)\n",
    "sr_psnr = psnr(hr_image, sr_image)\n",
    "hr_psnr = psnr(hr_image, hr_image)\n",
    "\n",
    "lr_image = lr_image.cpu()[0].numpy().transpose((1, 2, 0))\n",
    "hr_image = hr_image.cpu()[0].numpy().transpose((1, 2, 0))\n",
    "sr_image = sr_image.cpu()[0].numpy().transpose((1, 2, 0))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(f\"Low Resolution (Bicubic Scaling), PSNR {lr_psnr:.2f}\")\n",
    "plt.imshow(lr_image)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(f\"Super Resolution (CNN Scaling), PSNR {sr_psnr:.2f}\")\n",
    "plt.imshow(sr_image)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(f\"High Resolution (Label), PSNR {hr_psnr:.2f}\")\n",
    "plt.imshow(hr_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 12\n",
    "valid_images = valid_loader.dataset[index]\n",
    "\n",
    "lr_image = valid_images[0].unsqueeze(0).to(device)\n",
    "hr_image = valid_images[1].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr_image = model(valid_images[0].unsqueeze(0).to(device))\n",
    "\n",
    "lr_psnr = psnr(hr_image, lr_image)\n",
    "sr_psnr = psnr(hr_image, sr_image)\n",
    "hr_psnr = psnr(hr_image, hr_image)\n",
    "\n",
    "lr_image = lr_image.cpu()[0].numpy().transpose((1, 2, 0))\n",
    "hr_image = hr_image.cpu()[0].numpy().transpose((1, 2, 0))\n",
    "sr_image = sr_image.cpu()[0].numpy().transpose((1, 2, 0))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(f\"Low Resolution (Bicubic Scaling), PSNR {lr_psnr:.2f}\")\n",
    "plt.imshow(lr_image)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(f\"Super Resolution (CNN Scaling), PSNR {sr_psnr:.2f}\")\n",
    "plt.imshow(sr_image)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(f\"High Resolution (Label), PSNR {hr_psnr:.2f}\")\n",
    "plt.imshow(hr_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sets of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "SCALE = 4.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            image_data = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "            outputs = model(image_data)\n",
    "\n",
    "            outputs = denormalize(outputs)\n",
    "\n",
    "            # Calculate batch psnr (once every `batch_size` iterations).\n",
    "            batch_psnr = psnr(label, outputs)\n",
    "            running_psnr += batch_psnr\n",
    "\n",
    "    final_loss = running_loss / len(dataloader.dataset)\n",
    "    final_psnr = running_psnr / len(dataloader)\n",
    "    return final_loss, final_psnr\n",
    "\n",
    "\n",
    "# The SRCNN dataset module.\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The high resolution ground truth label.\n",
    "        label = Image.open(self.all_image_paths[index]).convert(\"RGB\")\n",
    "        w, h = label.size[:]\n",
    "\n",
    "        # Convert to 2x bicubic.\n",
    "        low_res_img = label.resize(\n",
    "            (int(w * (1.0 / SCALE)), int(h * (1.0 / SCALE))), Image.BICUBIC\n",
    "        )\n",
    "        # The low resolution input image.\n",
    "        image = low_res_img.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "\n",
    "        image /= 255.0\n",
    "        label /= 255.0\n",
    "\n",
    "        # normalize lr image\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = (image - mean) / std\n",
    "\n",
    "        image = image.transpose([2, 0, 1])\n",
    "        label = label.transpose([2, 0, 1])\n",
    "\n",
    "        return (\n",
    "            torch.tensor(image, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "\n",
    "# Prepare the datasets.\n",
    "def get_test_datasets(image_paths):\n",
    "    dataset_test = TestDataset(image_paths)\n",
    "    return dataset_test\n",
    "\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_test_dataloaders(dataset_test):\n",
    "    test_loader = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VGGSR1p().to(device)\n",
    "model.load_state_dict(torch.load(\"outputs/model.pth\"))\n",
    "data_paths = [[\"input/Set5/original\", \"Set5\"], [\"input/Set14/original\", \"Set14\"]]\n",
    "for data_path in data_paths:\n",
    "    dataset_test = get_test_datasets(data_path[0])\n",
    "    test_loader = get_test_dataloaders(dataset_test)\n",
    "    _, test_psnr = validate(model, test_loader, device)\n",
    "    print(f\"Test PSNR on {data_path[1]}: {test_psnr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
