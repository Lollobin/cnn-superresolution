{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjVWlSQm8TPi"
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Generate patches for training and low res images for validation and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubzGejQl8rsD"
   },
   "source": [
    "### Patch Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "error",
     "timestamp": 1728389383714,
     "user": {
      "displayName": "Benno Kossatz",
      "userId": "06085090293965508960"
     },
     "user_tz": -120
    },
    "id": "us157aRN8uAI",
    "outputId": "5b9d886a-38f8-41ea-ee6d-abfca3ea538f"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import patchify\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob as glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "SHOW_PATCHES = False\n",
    "STRIDE = 34\n",
    "SIZE = 64\n",
    "SCALE = 4.0  # Upscale factor 2, 3 or 4\n",
    "\n",
    "\n",
    "def show_patches(patches):\n",
    "    plt.figure(figsize=(patches.shape[0], patches.shape[1]))\n",
    "    gs = gridspec.GridSpec(patches.shape[0], patches.shape[1])\n",
    "    gs.update(wspace=0.01, hspace=0.02)\n",
    "    counter = 0\n",
    "    for i in range(patches.shape[0]):\n",
    "        for j in range(patches.shape[1]):\n",
    "            ax = plt.subplot(gs[counter])\n",
    "            plt.imshow(patches[i, j, 0, :, :, :])\n",
    "            plt.axis(\"off\")\n",
    "            counter += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_patches(\n",
    "    input_paths,\n",
    "    out_hr_path,\n",
    "    out_lr_path,\n",
    "):\n",
    "    os.makedirs(out_hr_path, exist_ok=True)\n",
    "    os.makedirs(out_lr_path, exist_ok=True)\n",
    "    all_paths = []\n",
    "    for input_path in input_paths:\n",
    "        all_paths.extend(glob.glob(f\"{input_path}/*\"))\n",
    "    print(f\"Creating patches for {len(all_paths)} images\")\n",
    "    for image_path in tqdm(all_paths, total=len(all_paths)):\n",
    "        image = Image.open(image_path)\n",
    "        image_name = image_path.split(os.path.sep)[-1].split(\".\")[0]\n",
    "        w, h = image.size\n",
    "        # Create patches of width and height SIZE.\n",
    "        patches = patchify.patchify(np.array(image), (SIZE, SIZE, 3), STRIDE)\n",
    "        if SHOW_PATCHES:\n",
    "            show_patches(patches)\n",
    "        counter = 0\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                counter += 1\n",
    "                patch = patches[i, j, 0, :, :, :]\n",
    "                patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(f\"{out_hr_path}/{image_name}_{counter}.png\", patch)\n",
    "                # Convert to bicubic and save.\n",
    "                h, w, _ = patch.shape\n",
    "                low_res_img = cv2.resize(\n",
    "                    patch,\n",
    "                    (int(w * (1.0 / SCALE)), int(h * 1.0 / SCALE)),\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                )\n",
    "                # Now upscale using BICUBIC.\n",
    "                high_res_upscale = cv2.resize(\n",
    "                    low_res_img, (w, h), interpolation=cv2.INTER_CUBIC\n",
    "                )\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_lr_path}/{image_name}_{counter}.png\", high_res_upscale\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_patches(\n",
    "    [\"input/T91\"], \"input/T91_hr_patches_64_x4\", \"input/T91_lr_patches_64_x4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bicubic Scaling for Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfGA9wyqAHH6"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob as glob\n",
    "import os\n",
    "\n",
    "paths = [\"input/Set14/original\", \"input/Set5/original\"]\n",
    "scale_factor = \"4x\"  # options 2x, 3x, 4x\n",
    "images = []\n",
    "\n",
    "for path in paths:\n",
    "    images.extend(glob.glob(f\"{path}/*.png\"))\n",
    "print(len(images))\n",
    "# Select scaling-factor and set up directories according to that.\n",
    "if scale_factor == \"2x\":\n",
    "    scale_factor = 0.5\n",
    "    os.makedirs(\"input/test_bicubic_rgb_2x\", exist_ok=True)\n",
    "    save_path_lr = \"input/test_bicubic_rgb_2x\"\n",
    "    os.makedirs(\"input/test_hr\", exist_ok=True)\n",
    "    save_path_hr = \"input/test_hr\"\n",
    "if scale_factor == \"3x\":\n",
    "    scale_factor = 0.333\n",
    "    os.makedirs(\"input/test_bicubic_rgb_3x\", exist_ok=True)\n",
    "    os.makedirs(\"input/test_hr\", exist_ok=True)\n",
    "    save_path_lr = \"input/test_bicubic_rgb_3x\"\n",
    "    save_path_hr = \"input/test_hr\"\n",
    "if scale_factor == \"4x\":\n",
    "    scale_factor = 0.25\n",
    "    os.makedirs(\"input/test_bicubic_rgb_4x\", exist_ok=True)\n",
    "    os.makedirs(\"input/test_hr\", exist_ok=True)\n",
    "    save_path_lr = \"input/test_bicubic_rgb_4x\"\n",
    "    save_path_hr = \"input/test_hr\"\n",
    "print(f\"Scaling factor: {scale_factor}\")\n",
    "print(f\"Low resolution images save path: {save_path_lr}\")\n",
    "for image in images:\n",
    "    orig_img = Image.open(image)\n",
    "    image_name = image.split(os.path.sep)[-1]\n",
    "    w, h = orig_img.size[:]\n",
    "    print(f\"Original image dimensions: {w}, {h}\")\n",
    "    orig_img.save(f\"{save_path_hr}/{image_name}\")\n",
    "    low_res_img = orig_img.resize(\n",
    "        (int(w * scale_factor), int(h * scale_factor)), Image.BICUBIC\n",
    "    )\n",
    "    # Upscale using BICUBIC.\n",
    "    high_res_upscale = low_res_img.resize((w, h), Image.BICUBIC)\n",
    "    high_res_upscale.save(f\"{save_path_lr}/{image_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df1rTJmG-dp-"
   },
   "source": [
    "## Utils\n",
    "\n",
    "Define utility functions that are used later on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hvJVv0i-o2P"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torchvision.utils import save_image\n",
    "from skimage.metrics import structural_similarity, peak_signal_noise_ratio\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "\n",
    "def psnr(label, outputs, max_val=1.0):\n",
    "    \"\"\"\n",
    "    Compute Peak Signal to Noise Ratio (the higher the better).\n",
    "    PSNR = 20 * log10(MAXp) - 10 * log10(MSE).\n",
    "    https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition\n",
    "    Note that the output and label pixels (when dealing with images) should\n",
    "    be normalized as the `max_val` here is 1 and not 255.\n",
    "    \"\"\"\n",
    "    label = label.cpu().detach().numpy()\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    diff = outputs - label\n",
    "    rmse = math.sqrt(np.mean((diff) ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    else:\n",
    "        PSNR = 20 * math.log10(max_val / rmse)\n",
    "        return PSNR\n",
    "\n",
    "\n",
    "def calculate_psnr(image_true, image_test, data_range=1):\n",
    "    image_true = (\n",
    "        image_true.cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    )  # Convert to (N, H, W, C)\n",
    "    image_test = (\n",
    "        image_test.cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "    )  # Convert to (N, H, W, C)\n",
    "    psnr_values = []\n",
    "    for i in range(image_true.shape[0]):\n",
    "        psnr_value = peak_signal_noise_ratio(\n",
    "            image_true[i], image_test[i], data_range=data_range\n",
    "        )\n",
    "        psnr_values.append(psnr_value)\n",
    "    return np.mean(psnr_values)\n",
    "\n",
    "\n",
    "# SSIM function using skimage\n",
    "def calculate_ssim(img1, img2):\n",
    "    img1 = img1.cpu().detach().numpy().transpose(0, 2, 3, 1)  # Convert to (N, H, W, C)\n",
    "    img2 = img2.cpu().detach().numpy().transpose(0, 2, 3, 1)  # Convert to (N, H, W, C)\n",
    "    ssim_values = []\n",
    "    for i in range(img1.shape[0]):\n",
    "        ssim_value = structural_similarity(\n",
    "            img1[i],\n",
    "            img2[i],\n",
    "            multichannel=True,\n",
    "            channel_axis=2,\n",
    "            data_range=img2[i].max() - img2[i].min(),\n",
    "        )\n",
    "        ssim_values.append(ssim_value)\n",
    "    return np.mean(ssim_values)\n",
    "\n",
    "\n",
    "def save_plot(train_loss, val_loss, train_psnr, val_psnr):\n",
    "\n",
    "    # Loss plots.\n",
    "    print(\"Saving loss plots...\")\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color=\"orange\", label=\"train loss\")\n",
    "    plt.plot(val_loss, color=\"red\", label=\"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"outputs/loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # PSNR plots.\n",
    "    print(\"Saving PSNR plots...\")\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_psnr, color=\"green\", label=\"train PSNR dB\")\n",
    "    plt.plot(val_psnr, color=\"blue\", label=\"validation PSNR dB\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"PSNR (dB)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"outputs/psnr.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_model_state(model):\n",
    "    # save the model to disk\n",
    "    print(\"Saving model state...\")\n",
    "    torch.save(model.state_dict(), \"outputs/model.pth\")\n",
    "\n",
    "\n",
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    # Remove the last model checkpoint if present.\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epochs + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": criterion,\n",
    "        },\n",
    "        f\"outputs/model_ckpt.pth\",\n",
    "    )\n",
    "\n",
    "\n",
    "def save_validation_results(outputs, epoch, batch_iter):\n",
    "    \"\"\"\n",
    "    Function to save the validation reconstructed images.\n",
    "    \"\"\"\n",
    "    save_image(outputs, f\"outputs/valid_results/val_sr_{epoch}_{batch_iter}.png\")\n",
    "\n",
    "\n",
    "### New Perceptual Loss Class ###\n",
    "\n",
    "\n",
    "# Perceptual Loss Class (Place under Util Section)\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers=None):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        # Use pre-trained VGG16 with batch normalization\n",
    "        self.vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT).features.eval()\n",
    "        # Freeze parameters\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Move to the appropriate device\n",
    "        self.vgg.to(device)\n",
    "        # Define the layers to use for perceptual loss\n",
    "        if layers is None:\n",
    "            self.layers = {\"relu1_1\": 2, \"relu2_1\": 9, \"relu3_1\": 16, \"relu4_1\": 23}\n",
    "        else:\n",
    "            self.layers = layers\n",
    "        # Loss function\n",
    "        self.criterion = nn.MSELoss()\n",
    "        # VGG normalization mean and std\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # Normalize input and target images\n",
    "        input = (input - self.mean) / self.std\n",
    "        target = (target - self.mean) / self.std\n",
    "        # Feature maps\n",
    "        input_features = {}\n",
    "        target_features = {}\n",
    "        x = input\n",
    "        y = target\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            y = layer(y)\n",
    "            if name in [str(i) for i in self.layers.values()]:\n",
    "                input_features[name] = x\n",
    "                target_features[name] = y\n",
    "        loss = 0.0\n",
    "        for name in input_features.keys():\n",
    "            loss += self.criterion(input_features[name], target_features[name])\n",
    "        return loss\n",
    "\n",
    "    # New end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob as glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "transform_image = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_label = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ResNetSRCNNDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "        self.all_label_paths = glob.glob(f\"{label_paths}/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.all_image_paths[index]).convert(\"RGB\")\n",
    "        label = Image.open(self.all_label_paths[index]).convert(\"RGB\")\n",
    "\n",
    "        image = transform_image(image)\n",
    "        label = transform_label(label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Prepare the datasets.\n",
    "def get_datasets(\n",
    "    train_image_paths, train_label_paths, valid_image_path, valid_label_paths\n",
    "):\n",
    "    dataset_train = ResNetSRCNNDataset(train_image_paths, train_label_paths)\n",
    "    dataset_valid = ResNetSRCNNDataset(valid_image_path, valid_label_paths)\n",
    "    return dataset_train, dataset_valid\n",
    "\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_dataloaders(dataset_train, dataset_valid):\n",
    "    train_loader = DataLoader(\n",
    "        dataset_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True, pin_memory=True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        dataset_valid, batch_size=TEST_BATCH_SIZE, shuffle=False, pin_memory=True\n",
    "    )\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img.numpy().transpose((1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)\n",
    "    plt.imshow(img)\n",
    "\n",
    "\n",
    "TRAIN_LABEL_PATHS = \"input/PIRM_hr_patches_224\"\n",
    "TRAN_IMAGE_PATHS = \"input/PIRM_lr_patches_224\"\n",
    "VALID_LABEL_PATHS = \"input/test_hr\"\n",
    "VALID_IMAGE_PATHS = \"input/test_bicubic_rgb_2x\"\n",
    "\n",
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS, VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "\n",
    "images = next(iter(train_loader))\n",
    "# images = next(iter(valid_loader))\n",
    "\n",
    "imshow(images[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSR1\n",
    "\n",
    "Contains 1 ResNet block in the encoder.\n",
    "\n",
    "Trainable parameters (decoder): 23.523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR1, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            # resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR1_m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR1_m, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=0, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                32, 16, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSR2\n",
    "\n",
    "Contains 2 ResNet blocks in the encoder.\n",
    "\n",
    "Trainable parameters (decoder): 97.315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR2, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "            resnet.layer2,  # Second residual block (with downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=0\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR2_m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR2_m, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual block (without downsampling)\n",
    "            resnet.layer2,  # Second residual block (with downsampling)\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSR3\n",
    "\n",
    "Contains 3 ResNet blocks in the encoder.\n",
    "\n",
    "Trainable parameters (decoder): 392.355"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Load a pretrained ResNet model\n",
    "resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class ResSR3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResSR3, self).__init__()\n",
    "\n",
    "        # Use only the initial layers of ResNet without downsampling\n",
    "        self.encoder = nn.Sequential(\n",
    "            resnet.conv1,  # First convolutional layer\n",
    "            resnet.bn1,  # Batch normalization\n",
    "            resnet.relu,  # Activation\n",
    "            # resnet.maxpool,  # Max pooling\n",
    "            resnet.layer1,  # First residual bloc\n",
    "            resnet.layer2,  # Second residual bloc\n",
    "            resnet.layer3,  # Third residual block\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 32, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 8\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=(1, 1), padding=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=(1, 1), padding=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=(1, 1), padding=(2, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR1\n",
    "\n",
    "Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR1, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR1p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR1p, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR1pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR1pn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR1pn, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 2\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR2, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "            vgg.features[13],  # Max Pooling\n",
    "            vgg.features[14],  # Convolutional layer\n",
    "            vgg.features[15],\n",
    "            vgg.features[16],\n",
    "            # vgg.features[17],\n",
    "            # vgg.features[18],\n",
    "            # vgg.features[19],\n",
    "            # vgg.features[20],\n",
    "            # vgg.features[21],\n",
    "            # vgg.features[22],\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                64, 3, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGSR2p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "from torch import nn\n",
    "\n",
    "vgg = vgg16_bn(weights=VGG16_BN_Weights.DEFAULT)\n",
    "\n",
    "\n",
    "class VGGSR2p(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGSR2p, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            vgg.features[0],  # Convolutional layer - 3\n",
    "            vgg.features[1],  # Batch Normalization\n",
    "            vgg.features[2],  # ReLU\n",
    "            vgg.features[3],  # Convolutional layer - 5\n",
    "            vgg.features[4],  # Batch Normalization\n",
    "            vgg.features[5],  # ReLU\n",
    "            vgg.features[6],  # Max Pooling - 10\n",
    "            vgg.features[7],  # Convolutional layer - 12\n",
    "            vgg.features[8],  # Batch Normalization\n",
    "            vgg.features[9],  # ReLU\n",
    "            vgg.features[10],  # Convolutional layer - 14\n",
    "            vgg.features[11],  # Batch Normalization\n",
    "            vgg.features[12],  # ReLU\n",
    "            vgg.features[13],  # Max Pooling\n",
    "            vgg.features[14],  # Convolutional layer\n",
    "            vgg.features[15],\n",
    "            vgg.features[16],\n",
    "        )\n",
    "\n",
    "        # SRCNN-inspired layers for feature extraction and reconstruction\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pad the input\n",
    "        original_size = x.size()[2:]\n",
    "        scale_factor = 4\n",
    "        pad_h = (scale_factor - original_size[0] % scale_factor) % scale_factor\n",
    "        pad_w = (scale_factor - original_size[1] % scale_factor) % scale_factor\n",
    "        padding = (0, pad_w, 0, pad_h)  # (left, right, top, bottom)\n",
    "        x = nn.functional.pad(x, padding, mode=\"reflect\")\n",
    "\n",
    "        # Feature extraction\n",
    "        features = self.encoder(x)\n",
    "\n",
    "        # Reconstruction\n",
    "        x = self.decoder(features)\n",
    "\n",
    "        # Remove padding\n",
    "        x = x[:, :, : original_size[0], : original_size[1]]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchscan import summary\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "model_dummy = VGGSR2p()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_dummy = model_dummy.to(device)\n",
    "\n",
    "for param in model_dummy.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Example forward pass with a dummy input\n",
    "dummy_input = torch.randn(1, 3, 224, 221).to(device)\n",
    "encoder_output = model_dummy.encoder(dummy_input)\n",
    "decoder_output = model_dummy.decoder(encoder_output)\n",
    "output = model_dummy(dummy_input)\n",
    "\n",
    "# Output shape should match the input shape\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
    "print(f\"Decoder output shape: {decoder_output.shape}\")\n",
    "print(f\"Final output shape: {output.shape}\")\n",
    "print(\n",
    "    f\"Number of parameters (encoder + decoder ): {count_parameters(model_dummy.encoder)}+{count_parameters(model_dummy.decoder)} = {count_parameters(model_dummy)}\"\n",
    ")\n",
    "\n",
    "\n",
    "summary(model_dummy, (3, 224, 221), receptive_field=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# Constants\n",
    "TRAIN_LABEL_PATHS = \"input/t91_hr_patches_32_x4\"\n",
    "TRAIN_IMAGE_PATHS = \"input/t91_lr_patches_32_x4\"\n",
    "VALID_LABEL_PATHS = \"input/test_hr\"\n",
    "VALID_IMAGE_PATHS = \"input/test_bicubic_rgb_4x\"\n",
    "\n",
    "dataset_train, dataset_valid = get_datasets(\n",
    "    TRAIN_IMAGE_PATHS, TRAIN_LABEL_PATHS, VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",
    ")\n",
    "train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n",
    "\n",
    "print(f\"Training samples: {len(dataset_train)}\")\n",
    "print(f\"Validation samples: {len(dataset_valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(1, 3, 1, 1).to(tensor.device)\n",
    "    std = torch.tensor(std).view(1, 3, 1, 1).to(tensor.device)\n",
    "    tensor = tensor * std + mean\n",
    "    # return torch.clamp(tensor, 0, 1)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Generic function to train a model\n",
    "# def train_model(model, criterion, optimizer=None, scheduler=None, num_epochs=10):\n",
    "def train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    perceptual_loss_fn,\n",
    "    optimizer=None,\n",
    "    scheduler=None,\n",
    "    num_epochs=10,\n",
    "    lambda_p=0.5,\n",
    "):\n",
    "    since = time.time()\n",
    "\n",
    "    # Copy weights\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_psnr = 0.0\n",
    "\n",
    "    train_loss, val_loss = [], []\n",
    "    train_psnr, val_psnr = [], []\n",
    "\n",
    "    lr_list = []\n",
    "    if scheduler is not None:\n",
    "        lr_list.append((1, scheduler.get_last_lr()[0]))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                # if scheduler is not None:\n",
    "                # scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "                dataloader = valid_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_psnr = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            \"\"\" for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                inputs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                if optimizer is not None:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history only if in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = denormalize(outputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_psnr += psnr(labels, outputs)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_psnr = running_psnr / len(dataloader)\n",
    "\n",
    "            print(\"{} Loss: {:.4f} PSNR: {:.4f}\".format(phase, epoch_loss, epoch_psnr)) \"\"\"\n",
    "\n",
    "            # New Integrating the Perceptual Loss into the Training Loop\n",
    "            for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "                inputs = data[0].to(device)\n",
    "                labels = data[1].to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                if optimizer is not None:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    outputs = denormalize(outputs)\n",
    "\n",
    "                    # Compute losses\n",
    "                    loss_pixel = criterion(outputs, labels)\n",
    "                    loss_perceptual = perceptual_loss_fn(outputs, labels)\n",
    "\n",
    "                    # Total loss\n",
    "                    loss = loss_pixel + lambda_p * loss_perceptual\n",
    "\n",
    "                    # Backward and optimize\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item()\n",
    "                running_psnr += calculate_psnr(labels, outputs)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_psnr = running_psnr / len(dataloader)\n",
    "\n",
    "            print(\n",
    "                \"{} Loss: {:.4f} PSNR: {:.4f} Pixel Loss: {:.4f} Perceptual Loss: {:.4f}\".format(\n",
    "                    phase,\n",
    "                    epoch_loss,\n",
    "                    epoch_psnr,\n",
    "                    loss_pixel.item(),\n",
    "                    loss_perceptual.item(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # New end\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_psnr > best_psnr:\n",
    "                best_psnr = epoch_psnr\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if phase == \"train\":\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_psnr.append(epoch_psnr)\n",
    "\n",
    "            if phase == \"val\":\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_psnr.append(epoch_psnr)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            last_lr = scheduler.get_last_lr()[0]\n",
    "            scheduler.step(epoch_loss)\n",
    "            new_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            if last_lr != new_lr:\n",
    "                print(\"LR changed from \", last_lr, \" to \", new_lr)\n",
    "                lr_list.append((epoch + 1, new_lr))\n",
    "\n",
    "        # save state and plots every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == num_epochs - 1:\n",
    "            save_model_state(model)\n",
    "            save_plot(train_loss, val_loss, train_psnr, val_psnr)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    print(\"Best val PSNR: {:4f}\".format(best_psnr))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # write PSNR values to text file for LaTeX\n",
    "    with open(\"outputs/psnr.txt\", \"w\") as f:\n",
    "        f.write(\"Train PSNR values: \\n\")\n",
    "        for epoch, psnr_value in enumerate(train_psnr):\n",
    "            f.write(f\"({epoch+1}, {psnr_value:.4f})\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Validation PSNR values: \\n\")\n",
    "        for epoch, psnr_value in enumerate(val_psnr):\n",
    "            f.write(f\"({epoch+1}, {psnr_value:.4f})\")\n",
    "\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"Learning rate values: \\n\")\n",
    "        for epoch, lr_value in lr_list:\n",
    "            f.write(f\"({epoch+1}, {lr_value})\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "model = VGGSR2p().to(device)\n",
    "# model.load_state_dict(torch.load(\"outputs/experiments_final/VGGSR_e150_model.pth\"))\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 150\n",
    "lr = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr, weight_decay=1e-4)\n",
    "\n",
    "scheduler = None\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#    optimizer=optimizer, mode=\"min\", factor=0.1, patience=25\n",
    "# )\n",
    "\n",
    "\n",
    "# Freeze the encoder layers\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Original Code\n",
    "# model = train_model(model, criterion, optimizer, scheduler, num_epochs=epochs)\n",
    "\n",
    "\n",
    "### New Intigrating Perceptual loss\n",
    "# Instantiate the perceptual loss function (Place in Training Section)\n",
    "perceptual_loss_fn = VGGPerceptualLoss().to(device)\n",
    "\n",
    "# Update the train_model function call\n",
    "model = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    perceptual_loss_fn,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=epochs,\n",
    "    lambda_p=0.0,\n",
    ")\n",
    "# New end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_state(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "model = VGGSR1pn().to(device)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"outputs/experiments_final_final/VGGSR1pn_e500_model_best.pth\")\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 50\n",
    "lr = 0.00001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "scheduler = None\n",
    "\n",
    "\n",
    "# Unfreeze the encoder layers\n",
    "\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "### New Intigrating Perceptual loss\n",
    "# Instantiate the perceptual loss function (Place in Training Section)\n",
    "perceptual_loss_fn = VGGPerceptualLoss().to(device)\n",
    "\n",
    "# Update the train_model function call\n",
    "model = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    perceptual_loss_fn,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    num_epochs=epochs,\n",
    "    lambda_p=0.3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_state(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading and upscaling a single image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "index = 5\n",
    "valid_images = valid_loader.dataset[index]\n",
    "\n",
    "lr_image = denormalize(valid_images[0].unsqueeze(0)).to(device)\n",
    "hr_image = valid_images[1].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sr_image = denormalize(model(valid_images[0].unsqueeze(0).to(device)))\n",
    "\n",
    "lr_psnr = calculate_psnr(hr_image, lr_image)\n",
    "sr_psnr = calculate_psnr(hr_image, sr_image)\n",
    "# hr_psnr = calculate_psnr(hr_image, hr_image)\n",
    "\n",
    "lr_ssim = calculate_ssim(hr_image, lr_image)\n",
    "sr_ssim = calculate_ssim(hr_image, sr_image)\n",
    "# hr_ssim = calculate_ssim(hr_image, hr_image)\n",
    "\n",
    "lr_image = lr_image.cpu()[0].numpy().transpose((1, 2, 0)).clip(0, 1)\n",
    "hr_image = hr_image.cpu()[0].numpy().transpose((1, 2, 0)).clip(0, 1)\n",
    "sr_image = sr_image.cpu()[0].numpy().transpose((1, 2, 0)).clip(0, 1)\n",
    "\n",
    "# Image.fromarray((lr_image*255).astype(np.uint8)).save(f\"outputs/images/{index}_lr_image_{lr_psnr:.2f}.png\")\n",
    "# Image.fromarray((hr_image*255).astype(np.uint8)).save(f\"outputs/images/{index}_hr_image_.png\")\n",
    "# Image.fromarray((sr_image*255).astype(np.uint8)).save(f\"outputs/images/{index}_sr_image_{sr_psnr:.2f}.png\")\n",
    "\n",
    "roi = [210, 20, 30, 30]  # [x, y, width, height]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(f\"Bicubic ({lr_psnr:.2f} / {lr_ssim:.3f})\")\n",
    "plt.imshow(lr_image, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "rect = plt.Rectangle(\n",
    "    (roi[0], roi[1]), roi[2], roi[3], edgecolor=\"red\", facecolor=\"none\", linewidth=2\n",
    ")\n",
    "plt.gca().add_patch(rect)\n",
    "axins = inset_axes(plt.gca(), width=2, height=2, loc=\"lower right\")\n",
    "axins.imshow(\n",
    "    lr_image[roi[1] : roi[1] + roi[3], roi[0] : roi[0] + roi[2]],\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(f\"VGGSR ({sr_psnr:.2f} / {sr_ssim:.3f})\")\n",
    "plt.imshow(sr_image, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "rect = plt.Rectangle(\n",
    "    (roi[0], roi[1]), roi[2], roi[3], edgecolor=\"red\", facecolor=\"none\", linewidth=2\n",
    ")\n",
    "plt.gca().add_patch(rect)\n",
    "axins = inset_axes(plt.gca(), width=2, height=2, loc=\"lower right\")\n",
    "axins.imshow(\n",
    "    sr_image[roi[1] : roi[1] + roi[3], roi[0] : roi[0] + roi[2]],\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(f\"Original (PSNR/SSIM)\")\n",
    "plt.imshow(hr_image, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "rect = plt.Rectangle(\n",
    "    (roi[0], roi[1]), roi[2], roi[3], edgecolor=\"red\", facecolor=\"none\", linewidth=2\n",
    ")\n",
    "plt.gca().add_patch(rect)\n",
    "axins = inset_axes(plt.gca(), width=2, height=2, loc=\"lower right\")\n",
    "axins.imshow(\n",
    "    hr_image[roi[1] : roi[1] + roi[3], roi[0] : roi[0] + roi[2]],\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "plt.savefig(\n",
    "    f\"outputs/images/{index}_comparison.pdf\", bbox_inches=\"tight\", pad_inches=0.2\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test sets of images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "SCALE = 4.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    running_psnr = 0.0\n",
    "    running_ssim = 0.0\n",
    "    running_bicubic_psnr = 0.0\n",
    "    running_bicubic_ssim = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            image = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "            outputs = model(image)\n",
    "\n",
    "            outputs = denormalize(outputs)\n",
    "\n",
    "            running_psnr += calculate_psnr(label, outputs)\n",
    "            running_ssim += calculate_ssim(label, outputs)\n",
    "\n",
    "            image = denormalize(image)\n",
    "            running_bicubic_psnr += calculate_psnr(label, image)\n",
    "            running_bicubic_ssim += calculate_ssim(label, image)\n",
    "\n",
    "    final_psnr = running_psnr / len(dataloader)\n",
    "    final_ssim = running_ssim / len(dataloader)\n",
    "    final_bicubic_psnr = running_bicubic_psnr / len(dataloader)\n",
    "    final_bicubic_ssim = running_bicubic_ssim / len(dataloader)\n",
    "    return final_psnr, final_ssim, final_bicubic_psnr, final_bicubic_ssim\n",
    "\n",
    "\n",
    "# The SRCNN dataset module.\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The high resolution ground truth label.\n",
    "        label = Image.open(self.all_image_paths[index]).convert(\"RGB\")\n",
    "        w, h = label.size[:]\n",
    "\n",
    "        # Convert to bicubic.\n",
    "        low_res_img = label.resize(\n",
    "            (int(w * (1.0 / SCALE)), int(h * (1.0 / SCALE))), Image.BICUBIC\n",
    "        )\n",
    "\n",
    "        # print(label.size[:], \" \", low_res_img.size[:])\n",
    "\n",
    "        # The low resolution input image.\n",
    "        image = low_res_img.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "\n",
    "        image /= 255.0\n",
    "        label /= 255.0\n",
    "\n",
    "        # normalize lr image\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = (image - mean) / std\n",
    "\n",
    "        image = image.transpose([2, 0, 1])\n",
    "        label = label.transpose([2, 0, 1])\n",
    "\n",
    "        return (\n",
    "            torch.tensor(image, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float),\n",
    "        )\n",
    "\n",
    "\n",
    "# Prepare the datasets.\n",
    "def get_test_datasets(image_paths):\n",
    "    dataset_test = TestDataset(image_paths)\n",
    "    return dataset_test\n",
    "\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_test_dataloaders(dataset_test):\n",
    "    test_loader = DataLoader(dataset_test, batch_size=1, shuffle=False)\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VGGSR2p().to(device)\n",
    "model.load_state_dict(torch.load(\"outputs/model.pth\"))\n",
    "data_paths = [[\"input/Set5/original\", \"Set5\"], [\"input/Set14/original\", \"Set14\"]]\n",
    "for data_path in data_paths:\n",
    "    dataset_test = get_test_datasets(data_path[0])\n",
    "    test_loader = get_test_dataloaders(dataset_test)\n",
    "    test_psnr, test_ssim, bicubic_psnr, bicubic_ssim = validate(\n",
    "        model, test_loader, device\n",
    "    )\n",
    "    print(f\"Test PSNR/SSIM on {data_path[1]}: {test_psnr:.3f}/{test_ssim:.3f}\")\n",
    "    print(\n",
    "        f\"Test PSNR/SSIM on {data_path[1]} (bicubic): {bicubic_psnr:.3f}/{bicubic_ssim:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigation into PSNR Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from PIL import Image\n",
    "from math import log10, sqrt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def custom_psnr(original, compressed, data_range=255.0):\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if mse == 0:  # MSE is zero means no noise is present in the signal .\n",
    "        # Therefore PSNR have no importance.\n",
    "        return 100\n",
    "    psnr = 20 * log10(data_range / sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "lr_path = \"input/Set5/LRbicx4/baby.png\"\n",
    "hr_path = \"input/Set5/GTmod12/baby.png\"\n",
    "\n",
    "hr_image = Image.open(hr_path).convert(\"RGB\")\n",
    "\n",
    "w, h = hr_image.size[:]\n",
    "\n",
    "lr_image_computed = hr_image.resize((w // 4, h // 4), Image.Resampling.BICUBIC)\n",
    "lr_image_computed = lr_image_computed.resize((w, h), Image.Resampling.BICUBIC)\n",
    "\n",
    "lr_image = Image.open(lr_path).convert(\"RGB\")\n",
    "lr_image_file = lr_image.resize((w, h), Image.Resampling.BICUBIC)\n",
    "\n",
    "hr_image = np.array(hr_image) / 255.0\n",
    "lr_image_computed = np.array(lr_image_computed) / 255.0\n",
    "lr_image_file = np.array(lr_image_file) / 255.0\n",
    "\n",
    "psnr_custom = custom_psnr(hr_image, lr_image_computed, data_range=1)\n",
    "psnr_skimage = peak_signal_noise_ratio(hr_image, lr_image_computed, data_range=1)\n",
    "\n",
    "psnr_custom_file = custom_psnr(hr_image, lr_image_file, data_range=1)\n",
    "psnr_skimage_file = peak_signal_noise_ratio(hr_image, lr_image_file, data_range=1)\n",
    "\n",
    "print(f\"PSNR custom: {psnr_custom:.2f}, {psnr_custom_file:.2f} \")\n",
    "print(f\"PSNR skimage: {psnr_skimage:.2f}, {psnr_skimage_file:.2f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
