{"cells":[{"cell_type":"markdown","metadata":{"id":"IABybSFt_Abf"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111703,"status":"ok","timestamp":1728397170072,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"OTR8hWUC-SX0","outputId":"34068e2d-6d7f-4dc9-da87-cb5cc6c1f886"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ux8qA-hA-Ud8","outputId":"c5761a5d-05e9-4872-a014-335d9ed734ae"},"outputs":[],"source":["working_directory = 'GitHub/dl-superresolution-ipynb'\n","%cd /content/drive/MyDrive/$working_directory\n","!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18342,"status":"ok","timestamp":1728388695293,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"dObxHyJH_f-Q","outputId":"0e6f4310-dcfe-4c81-fd63-e56b9cc943b0"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"0_sC6ggR_suO"},"source":["## Git Management"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EldXDCAATXb"},"outputs":[],"source":["!git config --global user.email \"e11909390@student.tuwien.ac.at\"\n","!git config --global user.name \"Lollobin\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11703,"status":"ok","timestamp":1728388290213,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"Oh3n9PMjBby0","outputId":"3a916314-3a9c-4e19-f7d9-3d16f79a85b0"},"outputs":[],"source":["!git pull"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1037,"status":"ok","timestamp":1728388879926,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"kgX9MTnv__xc","outputId":"24529da2-c3d3-4123-8022-ecabdfb1999f"},"outputs":[],"source":["!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4084,"status":"ok","timestamp":1728321821916,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"EXqwe0l9ABWA","outputId":"7f01249f-f40f-4a65-a21e-f30653f9cc89"},"outputs":[],"source":["!git commit -a -m \"added connection for google colab\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3046,"status":"ok","timestamp":1728321831755,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"6j8uohs5AMSU","outputId":"2f87b872-9427-43b6-b139-98941dbeec53"},"outputs":[],"source":["!git push"]},{"cell_type":"markdown","metadata":{},"source":["# Preparation"]},{"cell_type":"markdown","metadata":{"id":"GjVWlSQm8TPi"},"source":["## Data Preprocessing\n","\n","Generate patches for training and low res images for validation and testing."]},{"cell_type":"markdown","metadata":{"id":"ubzGejQl8rsD"},"source":["### Patch Generation"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405},"executionInfo":{"elapsed":468,"status":"error","timestamp":1728389383714,"user":{"displayName":"Benno Kossatz","userId":"06085090293965508960"},"user_tz":-120},"id":"us157aRN8uAI","outputId":"5b9d886a-38f8-41ea-ee6d-abfca3ea538f"},"outputs":[],"source":["from PIL import Image\n","from tqdm import tqdm\n","\n","import matplotlib.pyplot as plt\n","import patchify\n","import numpy as np\n","import matplotlib.gridspec as gridspec\n","import glob as glob\n","import os\n","import cv2\n","\n","SHOW_PATCHES = False\n","STRIDE = 14\n","SIZE = 32\n","\n","def show_patches(patches):\n","    plt.figure(figsize=(patches.shape[0], patches.shape[1]))\n","    gs = gridspec.GridSpec(patches.shape[0], patches.shape[1])\n","    gs.update(wspace=0.01, hspace=0.02)\n","    counter = 0\n","    for i in range(patches.shape[0]):\n","        for j in range(patches.shape[1]):\n","            ax = plt.subplot(gs[counter])\n","            plt.imshow(patches[i, j, 0, :, :, :])\n","            plt.axis('off')\n","            counter += 1\n","    plt.show()\n","\n","def create_patches(\n","    input_paths, out_hr_path, out_lr_path,\n","):\n","    os.makedirs(out_hr_path, exist_ok=True)\n","    os.makedirs(out_lr_path, exist_ok=True)\n","    all_paths = []\n","    for input_path in input_paths:\n","        all_paths.extend(glob.glob(f\"{input_path}/*\"))\n","    print(f\"Creating patches for {len(all_paths)} images\")\n","    for image_path in tqdm(all_paths, total=len(all_paths)):\n","        image = Image.open(image_path)\n","        image_name = image_path.split(os.path.sep)[-1].split('.')[0]\n","        w, h = image.size\n","        # Create patches of size (32, 32, 3)\n","        patches = patchify.patchify(np.array(image), (32, 32, 3), STRIDE)\n","        if SHOW_PATCHES:\n","            show_patches(patches)\n","        counter = 0\n","        for i in range(patches.shape[0]):\n","            for j in range(patches.shape[1]):\n","                counter += 1\n","                patch = patches[i, j, 0, :, :, :]\n","                patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n","                cv2.imwrite(\n","                    f\"{out_hr_path}/{image_name}_{counter}.png\",\n","                    patch\n","                )\n","                # Convert to bicubic and save.\n","                h, w, _ = patch.shape\n","                low_res_img = cv2.resize(patch, (int(w*0.5), int(h*0.5)),\n","                                        interpolation=cv2.INTER_CUBIC)\n","                # Now upscale using BICUBIC.\n","                high_res_upscale = cv2.resize(low_res_img, (w, h),\n","                                            interpolation=cv2.INTER_CUBIC)\n","                cv2.imwrite(\n","                    f\"{out_lr_path}/{image_name}_{counter}.png\",\n","                    high_res_upscale\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["create_patches(['input/T91'], 'input/t91_hr_patches', 'input/t91_lr_patches')"]},{"cell_type":"markdown","metadata":{},"source":["### Bicubic Scaling for Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zfGA9wyqAHH6"},"outputs":[],"source":["from PIL import Image\n","import glob as glob\n","import os\n","\n","paths = ['input/Set14/original', 'input/Set5/original']\n","scale_factor = '2x' # options 2x, 3x, 4x\n","images = []\n","\n","for path in paths:\n","    images.extend(glob.glob(f\"{path}/*.png\"))\n","print(len(images))\n","# Select scaling-factor and set up directories according to that.\n","if scale_factor == '2x':\n","    scale_factor = 0.5\n","    os.makedirs('input/test_bicubic_rgb_2x', exist_ok=True)\n","    save_path_lr = 'input/test_bicubic_rgb_2x'\n","    os.makedirs('input/test_hr', exist_ok=True)\n","    save_path_hr = 'input/test_hr'\n","if scale_factor == '3x':\n","    scale_factor = 0.333\n","    os.makedirs('input/test_bicubic_rgb_3x', exist_ok=True)\n","    os.makedirs('input/test_hr', exist_ok=True)\n","    save_path_lr = 'input/test_bicubic_rgb_3x'\n","    save_path_hr = 'input/test_hr'\n","if scale_factor == '4x':\n","    scale_factor = 0.25\n","    os.makedirs('input/test_bicubic_rgb_4x', exist_ok=True)\n","    os.makedirs('input/test_hr', exist_ok=True)\n","    save_path_lr = 'input/test_bicubic_rgb_4x'\n","    save_path_hr = 'input/test_hr'\n","print(f\"Scaling factor: {scale_factor}\")\n","print(f\"Low resolution images save path: {save_path_lr}\")\n","for image in images:\n","    orig_img = Image.open(image)\n","    image_name = image.split(os.path.sep)[-1]\n","    w, h = orig_img.size[:]\n","    print(f\"Original image dimensions: {w}, {h}\")\n","    orig_img.save(f\"{save_path_hr}/{image_name}\")\n","    low_res_img = orig_img.resize((int(w*scale_factor), int(h*scale_factor)), Image.BICUBIC)\n","    # Upscale using BICUBIC.\n","    high_res_upscale = low_res_img.resize((w, h), Image.BICUBIC)\n","    high_res_upscale.save(f\"{save_path_lr}/{image_name}\")"]},{"cell_type":"markdown","metadata":{"id":"df1rTJmG-dp-"},"source":["## Utils\n","\n","Define utility functions that are used later on."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"7hvJVv0i-o2P"},"outputs":[],"source":["import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from torchvision.utils import save_image\n","plt.style.use('ggplot')\n","def psnr(label, outputs, max_val=1.):\n","    \"\"\"\n","    Compute Peak Signal to Noise Ratio (the higher the better).\n","    PSNR = 20 * log10(MAXp) - 10 * log10(MSE).\n","    https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Definition\n","    Note that the output and label pixels (when dealing with images) should\n","    be normalized as the `max_val` here is 1 and not 255.\n","    \"\"\"\n","    label = label.cpu().detach().numpy()\n","    outputs = outputs.cpu().detach().numpy()\n","    diff = outputs - label\n","    rmse = math.sqrt(np.mean((diff) ** 2))\n","    if rmse == 0:\n","        return 100\n","    else:\n","        PSNR = 20 * math.log10(max_val / rmse)\n","        return PSNR\n","\n","def save_plot(train_loss, val_loss, train_psnr, val_psnr):\n","    # Loss plots.\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(train_loss, color='orange', label='train loss')\n","    plt.plot(val_loss, color='red', label='validataion loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig('outputs/loss.png')\n","    plt.close()\n","    # PSNR plots.\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(train_psnr, color='green', label='train PSNR dB')\n","    plt.plot(val_psnr, color='blue', label='validataion PSNR dB')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('PSNR (dB)')\n","    plt.legend()\n","    plt.savefig('outputs/psnr.png')\n","    plt.close()\n","\n","def save_model_state(model):\n","    # save the model to disk\n","    print('Saving model...')\n","    torch.save(model.state_dict(), 'outputs/model.pth')\n","\n","def save_model(epochs, model, optimizer, criterion):\n","    \"\"\"\n","    Function to save the trained model to disk.\n","    \"\"\"\n","    # Remove the last model checkpoint if present.\n","    torch.save({\n","                'epoch': epochs+1,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'loss': criterion,\n","                }, f\"outputs/model_ckpt.pth\")\n","\n","def save_validation_results(outputs, epoch, batch_iter):\n","    \"\"\"\n","    Function to save the validation reconstructed images.\n","    \"\"\"\n","    save_image(\n","        outputs,\n","        f\"outputs/valid_results/val_sr_{epoch}_{batch_iter}.png\"\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["# Data Loading"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","import glob as glob\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","\n","TRAIN_BATCH_SIZE = 128\n","TEST_BATCH_SIZE = 1\n","\n","# The SRCNN dataset module.\n","class SRCNNDataset(Dataset):\n","    def __init__(self, image_paths, label_paths):\n","        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n","        self.all_label_paths = glob.glob(f\"{label_paths}/*\") \n","    \n","    def __len__(self):\n","        return (len(self.all_image_paths))\n","    \n","    def __getitem__(self, index):\n","        image = Image.open(self.all_image_paths[index]).convert('RGB')\n","        label = Image.open(self.all_label_paths[index]).convert('RGB')\n","        image = np.array(image, dtype=np.float32)\n","        label = np.array(label, dtype=np.float32)\n","        image /= 255.\n","        label /= 255.\n","        image = image.transpose([2, 0, 1])\n","        label = label.transpose([2, 0, 1])\n","        return (\n","            torch.tensor(image, dtype=torch.float),\n","            torch.tensor(label, dtype=torch.float)\n","        )\n","    \n","# Prepare the datasets.\n","def get_datasets(\n","    train_image_paths, train_label_paths,\n","    valid_image_path, valid_label_paths\n","):\n","    dataset_train = SRCNNDataset(\n","        train_image_paths, train_label_paths\n","    )\n","    dataset_valid = SRCNNDataset(\n","        valid_image_path, valid_label_paths\n","    )\n","    return dataset_train, dataset_valid\n","\n","# Prepare the data loaders\n","def get_dataloaders(dataset_train, dataset_valid):\n","    train_loader = DataLoader(\n","        dataset_train, \n","        batch_size=TRAIN_BATCH_SIZE,\n","        shuffle=True\n","    )\n","    valid_loader = DataLoader(\n","        dataset_valid, \n","        batch_size=TEST_BATCH_SIZE,\n","        shuffle=False\n","    )\n","    return train_loader, valid_loader"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","import glob as glob\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image\n","from torchvision import transforms\n","\n","TRAIN_BATCH_SIZE = 128\n","TEST_BATCH_SIZE = 1\n","\n","class ResNetSRCNNDataset(Dataset):\n","    def __init__(self, image_paths, label_paths, transform=None):\n","        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n","        self.all_label_paths = glob.glob(f\"{label_paths}/*\")\n","        self.transform = transform \n","    \n","    def __len__(self):\n","        return (len(self.all_image_paths))\n","    \n","    def __getitem__(self, index):\n","        image = Image.open(self.all_image_paths[index]).convert('RGB')\n","        label = Image.open(self.all_label_paths[index]).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","            label = self.transform(label)\n","\n","        return image, label\n","    \n","\n","class ResizeToEven:\n","    def __call__(self, image):\n","        # Get current size\n","        width, height = image.size\n","        \n","        # If the width or height is odd, reduce by one pixel\n","        new_width = width if width % 2 == 0 else width - 1\n","        new_height = height if height % 2 == 0 else height - 1\n","        \n","        # Resize the image if needed\n","        if new_width != width or new_height != height:\n","            image = image.resize((new_width, new_height), Image.BICUBIC)\n","        \n","        return image\n","\n","\n","\n","preprocess_train =  transforms.Compose([\n","        #transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n","        ResizeToEven(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","\n","preprocess_valid =  transforms.Compose([\n","        #transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n","        ResizeToEven(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","    ])\n","    \n","# Prepare the datasets.\n","def get_datasets(\n","    train_image_paths, train_label_paths,\n","    valid_image_path, valid_label_paths\n","):\n","    dataset_train = ResNetSRCNNDataset(\n","        train_image_paths, train_label_paths, transform=preprocess_train\n","    )\n","    dataset_valid = ResNetSRCNNDataset(\n","        valid_image_path, valid_label_paths, transform=preprocess_valid\n","    )\n","    return dataset_train, dataset_valid\n","\n","# Prepare the data loaders\n","def get_dataloaders(dataset_train, dataset_valid):\n","    train_loader = DataLoader(\n","        dataset_train, \n","        batch_size=TRAIN_BATCH_SIZE,\n","        shuffle=True\n","    )\n","    valid_loader = DataLoader(\n","        dataset_valid, \n","        batch_size=TEST_BATCH_SIZE,\n","        shuffle=False\n","    )\n","    return train_loader, valid_loader\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision import utils\n","\n","def imshow(img):\n","    img = img.numpy().transpose((1,2,0))\n","\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    img = std * img + mean\n","    img = np.clip(img, 0, 1)\n","\n","    plt.imshow(img)\n","\n","TRAIN_LABEL_PATHS = 'input/t91_hr_patches'\n","TRAN_IMAGE_PATHS = 'input/t91_lr_patches'\n","VALID_LABEL_PATHS = 'input/test_hr'\n","VALID_IMAGE_PATHS = 'input/test_bicubic_rgb_2x'\n","\n","dataset_train, dataset_valid = get_datasets(\n","    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS,\n","    VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",")\n","train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n","\n","#images = next(iter(train_loader))\n","images = next(iter(valid_loader))\n","\n","#out = utils.make_grid(images)\n","imshow(images[1][0])"]},{"cell_type":"markdown","metadata":{},"source":["# Network Definition"]},{"cell_type":"markdown","metadata":{},"source":["## SRCNN"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SRCNN(nn.Module):\n","    def __init__(self):\n","        super(SRCNN, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(\n","            3, 64, kernel_size=9, stride=(1, 1), padding=(2, 2)\n","        )\n","        self.conv2 = nn.Conv2d(\n","            64, 32, kernel_size=1, stride=(1, 1), padding=(2, 2)\n","        )\n","        self.conv3 = nn.Conv2d(\n","            32, 3, kernel_size=5, stride=(1, 1), padding=(2, 2)\n","        )\n","    \n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.conv3(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## ResNet SRCNN Combination"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import resnet18, ResNet18_Weights\n","\n","# Load a pretrained ResNet model\n","resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n","\n","class ResNetSRCNN(nn.Module):\n","    def __init__(self):\n","        super(ResNetSRCNN, self).__init__()\n","        \n","        # Use only the initial layers of ResNet without downsampling\n","        self.resnet_layers = nn.Sequential(\n","            resnet.conv1,  # First convolutional layer\n","            resnet.bn1,    # Batch normalization\n","            resnet.relu,   # Activation\n","            resnet.layer1  # First residual block (without downsampling)\n","        )\n","        \n","        # Freeze the ResNet layers\n","        for param in self.resnet_layers.parameters():\n","            param.requires_grad = False\n","        \n","        # SRCNN-inspired layers for feature extraction and reconstruction\n","        self.reconstruction = nn.Sequential(\n","            nn.ConvTranspose2d(64,64,kernel_size=4,stride=2,padding=1), # Upscaling to keep input and output size the same\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Keep the spatial size same\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(32, 3, kernel_size=3, padding=1),   # Output RGB channels\n","        )\n","\n","    def forward(self, x):\n","        # Feature extraction (ResNet initial layers)\n","        features = self.resnet_layers(x)\n","        \n","        # Reconstruction (SRCNN-inspired layers)\n","        x = self.reconstruction(features)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the model\n","resNetSRCNN = ResNetSRCNN()\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","resNetSRCNN = resNetSRCNN.to(device)\n","\n","# Example forward pass with a dummy input\n","dummy_input = torch.randn(1, 3, 18,18).to(device)\n","\n","resnet_output = resNetSRCNN.resnet_layers(dummy_input)\n","output = resNetSRCNN(dummy_input)\n","\n","# Output shape should match the input shape\n","print(f\"Input shape: {dummy_input.shape}\")\n","print(f\"ResNet output shape: {resnet_output.shape}\")\n","print(f\"Final output shape: {output.shape}\")\n","#print(resNetSRCNN)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import time\n","import torch.optim as optim\n","import torch.nn as nn\n","import os\n","from tqdm import tqdm\n","\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Learning parameters.\n","epochs = 10 # Number of epochs to train the SRCNN model for.\n","lr = 0.001 # Learning rate.\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","weights = None # weights/checkpoint path to resume training\n","\n","# Constants\n","TRAIN_LABEL_PATHS = 'input/t91_hr_patches'\n","TRAN_IMAGE_PATHS = 'input/t91_lr_patches'\n","VALID_LABEL_PATHS = 'input/test_hr'\n","VALID_IMAGE_PATHS = 'input/test_bicubic_rgb_2x'\n","SAVE_VALIDATION_RESULTS = True\n","\n","os.makedirs('outputs/valid_results', exist_ok=True)\n","\n","# Initialize the model.\n","print('Computation device: ', device)\n","model = ResNetSRCNN().to(device)\n","if weights is not None:\n","    print('Loading weights to resume training...')\n","    checkpoint = torch.load(weights)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","print(model)\n","\n","# Optimizer.\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","# Loss function. \n","criterion = nn.MSELoss()\n","\n","dataset_train, dataset_valid = get_datasets(\n","    TRAN_IMAGE_PATHS, TRAIN_LABEL_PATHS,\n","    VALID_IMAGE_PATHS, VALID_LABEL_PATHS\n",")\n","train_loader, valid_loader = get_dataloaders(dataset_train, dataset_valid)\n","\n","print(f\"Training samples: {len(dataset_train)}\")\n","print(f\"Validation samples: {len(dataset_valid)}\")\n","\n","def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n","    mean = torch.tensor(mean).view(1, 3, 1, 1).to(tensor.device)\n","    std = torch.tensor(std).view(1, 3, 1, 1).to(tensor.device)\n","    return tensor * std + mean\n","\n","def train(model, dataloader):\n","    model.train()\n","    running_loss = 0.0\n","    running_psnr = 0.0\n","    for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n","        image_data = data[0].to(device)\n","        label = data[1].to(device)\n","        \n","        # Zero grad the optimizer.\n","        optimizer.zero_grad()\n","        outputs = model(image_data)\n","\n","        outputs_denorm = denormalize(outputs)\n","\n","        loss = criterion(outputs_denorm, label)\n","        \n","        # Backpropagation.\n","        loss.backward()\n","        # Update the parameters.\n","        optimizer.step()\n","        \n","        # Add loss of each item (total items in a batch = batch size).\n","        running_loss += loss.item()\n","        # Calculate batch psnr (once every `batch_size` iterations).\n","        batch_psnr =  psnr(label, outputs_denorm)\n","        running_psnr += batch_psnr\n","    \n","    final_loss = running_loss/len(dataloader.dataset)\n","    final_psnr = running_psnr/len(dataloader)\n","    return final_loss, final_psnr\n","\n","def validate(model, dataloader, epoch):\n","    model.eval()\n","    running_loss = 0.0\n","    running_psnr = 0.0\n","    with torch.no_grad():\n","        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n","            image_data = data[0].to(device)\n","            label = data[1].to(device)\n","            \n","            outputs = model(image_data)\n","            outputs_denorm = denormalize(outputs)\n","            loss = criterion(outputs_denorm, label)\n","            \n","            # Add loss of each item (total items in a batch = batch size) .\n","            running_loss += loss.item()\n","            \n","            # Calculate batch psnr (once every `batch_size` iterations).\n","            batch_psnr = psnr(label, outputs_denorm)\n","            running_psnr += batch_psnr\n","            \n","            # For saving the batch samples for the validation results\n","            # every 500 epochs.\n","            if SAVE_VALIDATION_RESULTS and (epoch % 500) == 0:\n","                save_validation_results(outputs_denorm, epoch, bi)\n","    final_loss = running_loss/len(dataloader.dataset)\n","    final_psnr = running_psnr/len(dataloader)\n","    return final_loss, final_psnr\n","\n","train_loss, val_loss = [], []\n","train_psnr, val_psnr = [], []\n","start = time.time()\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1} of {epochs}\")\n","    train_epoch_loss, train_epoch_psnr = train(model, train_loader)\n","    val_epoch_loss, val_epoch_psnr = validate(model, valid_loader, epoch+1)\n","    print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n","    print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n","    train_loss.append(train_epoch_loss)\n","    train_psnr.append(train_epoch_psnr)\n","    val_loss.append(val_epoch_loss)\n","    val_psnr.append(val_epoch_psnr)\n","    \n","    # Save model with all information every 100 epochs. Can be used \n","    # resuming training.\n","    if (epoch+1) % 100 == 0:\n","        save_model(epoch, model, optimizer, criterion)\n","    \n","    # Save the model state dictionary only every epoch. Small size, \n","    # can be used for inference.\n","    save_model_state(model)\n","    \n","    # Save the PSNR and loss plots every epoch.\n","    save_plot(train_loss, val_loss, train_psnr, val_psnr)\n","end = time.time()\n","print(f\"Finished training in: {((end-start)/60):.3f} minutes\") "]},{"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"markdown","metadata":{},"source":["Test loading and upscaling a single image."]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[],"source":["train_images = next(iter(train_loader))\n","valid_images = next(iter(valid_loader))\n","\n","def np_denormalize(img):\n","    img = img.numpy().transpose((1,2,0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    img = std * img + mean\n","    img = np.clip(img, 0, 1)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(valid_images))\n","print(valid_images[0].shape)\n","\n","lr_image = np_denormalize(valid_images[0][0])\n","hr_image = np_denormalize(valid_images[1][0])\n","\n","with torch.no_grad():\n","    sr_image = model(valid_images[0].to(device)).cpu()[0]\n","sr_image = np_denormalize(sr_image)\n","\n","plt.figure(figsize=(20,10))\n","plt.subplot(1,3,1)\n","plt.title(\"Low Resolution (Bicubic Scaling)\")\n","plt.imshow(lr_image)\n","plt.subplot(1,3,2)\n","plt.title(\"Super Resolution (CNN Scaling)\")\n","plt.imshow(sr_image)\n","plt.subplot(1,3,3)\n","plt.title(\"High Resolution (Label)\")\n","plt.imshow(hr_image)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["Actual testing is currently broken because of the rescaling performed in the preprocessing inside the model."]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["import torch\n","import glob as glob\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from PIL import Image\n","from torch.utils.data import DataLoader, Dataset\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def validate(model, dataloader, device):\n","    model.eval()\n","    running_loss = 0.0\n","    running_psnr = 0.0\n","    with torch.no_grad():\n","        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n","            image_data = data[0].to(device)\n","            label = data[1].to(device)\n","            outputs = model(image_data)\n","            # Calculate batch psnr (once every `batch_size` iterations).\n","            batch_psnr = psnr(label, outputs)\n","            running_psnr += batch_psnr\n","\n","    final_loss = running_loss/len(dataloader.dataset)\n","    final_psnr = running_psnr/len(dataloader)\n","    return final_loss, final_psnr\n","\n","# The SRCNN dataset module.\n","class SRCNNDataset(Dataset):\n","    def __init__(self, image_paths):\n","        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n","\n","    def __len__(self):\n","        return (len(self.all_image_paths))\n","\n","    def __getitem__(self, index):\n","        # The high resolution ground truth label.\n","        label = Image.open(self.all_image_paths[index]).convert('RGB')\n","        w, h = label.size[:]\n","        # Convert to 2x bicubic.\n","        low_res_img = label.resize((int(w*0.5), int(h*0.5)), Image.BICUBIC)\n","        # The low resolution input image.\n","        image = low_res_img.resize((w, h), Image.BICUBIC)\n","\n","        # Uncomment the below code to visualize the image for sanity check.\n","        #plt.figure(figsize=(12, 9))\n","        #plt.subplot(1, 2, 1)\n","        #plt.imshow(image)\n","        #plt.axis('off')\n","        #plt.subplot(1, 2, 2)\n","        #plt.imshow(label)\n","        #plt.axis('off')\n","        #plt.show()\n","\n","        image = np.array(image, dtype=np.float32)\n","        label = np.array(label, dtype=np.float32)\n","\n","        image /= 255.\n","        label /= 255.\n","\n","        image = image.transpose([2, 0, 1])\n","        label = label.transpose([2, 0, 1])\n","\n","        return (\n","            torch.tensor(image, dtype=torch.float),\n","            torch.tensor(label, dtype=torch.float)\n","        )\n","\n","# Prepare the datasets.\n","def get_datasets(\n","    image_paths\n","):\n","    dataset_test = SRCNNDataset(image_paths)\n","    return dataset_test\n","\n","# Prepare the data loaders\n","def get_dataloaders(dataset_test):\n","    test_loader = DataLoader(\n","        dataset_test, \n","        batch_size=1,\n","        shuffle=False\n","    )\n","    return test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the model.\n","model = ResNetSRCNN().to(device)\n","model.load_state_dict(torch.load('outputs/model.pth'))\n","data_paths = [\n","    ['input/Set5/original', 'Set5'],\n","    ['input/Set14/original', 'Set14']\n","]\n","for data_path in data_paths:\n","    dataset_test = get_datasets(data_path[0])\n","    test_loader = get_dataloaders(dataset_test)\n","    _, test_psnr = validate(model, test_loader, device)\n","    print(f\"Test PSNR on {data_path[1]}: {test_psnr:.3f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}
